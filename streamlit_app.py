#!/usr/bin/env python3
"""
MediAid AI - Interactive Medical Search Interface
Streamlit web app for searching medical information from CDC and WHO databases
"""

import streamlit as st
import os
import sys
import re
import json
import traceback
from datetime import datetime
from typing import List, Dict, Optional
from dotenv import load_dotenv
from pathlib import Path
from llama_index.core import Settings, VectorStoreIndex, Document
from llama_index.llms.openai import OpenAI as LIOpenAI
from llama_index.embeddings.openai import OpenAIEmbedding
import os

# Load .env next to this file (reliable)
load_dotenv(dotenv_path=Path(__file__).parent / ".env")

print("API Key Loaded?", bool(os.getenv("OPENAI_API_KEY")))

Settings.llm = LIOpenAI(model="gpt-3.5-turbo", temperature=0.1, api_key=os.getenv("OPENAI_API_KEY"))
Settings.embed_model = OpenAIEmbedding(model="text-embedding-3-small", api_key=os.getenv("OPENAI_API_KEY"))

docs = [Document(text="Diabetes mellitus is a metabolic disease with high blood glucose."),
        Document(text="Hypertension is high blood pressure; lifestyle and medication help.")]

idx = VectorStoreIndex.from_documents(docs)
qe = idx.as_query_engine(similarity_top_k=2, response_mode="tree_summarize")
print(qe.query("What is diabetes?"))  # should print a coherent answer

# Add utils to path
sys.path.append(os.path.join(os.path.dirname(__file__), 'utils'))

# Configure Streamlit page
st.set_page_config(
    page_title="MediAid AI - Medical Search",
    page_icon="ğŸ¥",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Initialize session state
if 'current_page' not in st.session_state:
    st.session_state.current_page = 'home'
if 'selected_disease' not in st.session_state:
    st.session_state.selected_disease = None
if 'search_query' not in st.session_state:
    st.session_state.search_query = ""
if 'chat_history' not in st.session_state:
    st.session_state.chat_history = []
if 'chat_context' not in st.session_state:
    st.session_state.chat_context = []
if 'authenticated' not in st.session_state:
    st.session_state.authenticated = False
if 'username' not in st.session_state:
    st.session_state.username = ""

@st.cache_resource
def load_medical_search():
    """Load the medical search system"""
    try:
        from utils.faiss_utils import FAISSVectorStore
        
        vector_store = FAISSVectorStore()
        vector_store.load_index("rag_data/medical_embeddings.index")
        
        return vector_store
    except Exception as e:
        st.error(f"Failed to load medical database: {e}")
        return None

@st.cache_resource
def load_llamaindex_search():
    """Load LlamaIndex search system (simple implementation)"""
    try:
        import os, json
        from llama_index.core import VectorStoreIndex, Document, Settings
        # Configure LlamaIndex with OpenAI (v0.10+ requires this)
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key or api_key == "your-api-key-here":
            st.warning("âš ï¸ LlamaIndex requires OPENAI_API_KEY. Skipping.")
            return None

        # Set default LLM + embeddings
        from llama_index.llms.openai import OpenAI as LIOpenAI
        from llama_index.embeddings.openai import OpenAIEmbedding
        Settings.llm = LIOpenAI(model="gpt-3.5-turbo", temperature=0.1)
        Settings.embed_model = OpenAIEmbedding(model="text-embedding-3-small")

        # Build documents (keep your simple loader)
        documents = []
        who_path = "rag_data/embedded/who_embeddings.json"
        if os.path.exists(who_path):
            with open(who_path, "r", encoding="utf-8") as f:
                for item in json.load(f)[:100]:
                    documents.append(Document(text=item.get("text",""),
                                              metadata={"source":"WHO", **item.get("metadata",{})}))
        cdc_path = "rag_data/embedded/cdc_embeddings.json"
        if os.path.exists(cdc_path):
            with open(cdc_path, "r", encoding="utf-8") as f:
                for item in json.load(f)[:100]:
                    documents.append(Document(text=item.get("text",""),
                                              metadata={"source":"CDC", **item.get("metadata",{})}))

        if not documents:
            st.warning("No medical documents found for LlamaIndex")
            return None

        index = VectorStoreIndex.from_documents(documents)
        query_engine = index.as_query_engine(
            similarity_top_k=5,
            response_mode="tree_summarize",
            verbose=False,
        )
        st.success(f"âœ… LlamaIndex loaded {len(documents)} medical documents")
        return query_engine

    except ImportError:
        st.warning("âš ï¸ LlamaIndex not installed. Run: pip install llama-index")
        return None
    except Exception as e:
        st.error(f"Failed to load LlamaIndex: {e}")
        return None


@st.cache_data
def get_medical_topics(_vector_store):
    """Get list of available medical topics"""
    if not _vector_store:
        return []
    
    topics = set()
    for meta in _vector_store.metadata:
        text = meta.get('text', '')
        if 'Disease:' in text:
            topic = text.split('Disease:')[1].split('\n')[0].strip()
            if topic and len(topic) > 0:
                topics.add(topic)
    
    return sorted(list(topics))

def search_documents_llamaindex(llamaindex_engine, query: str) -> str:
    """Search using LlamaIndex query engine with enhanced prompting"""
    if not llamaindex_engine:
        return None
    
    try:
        # Enhanced query with detailed prompting for comprehensive medical responses
        enhanced_query = f"""
        Based on the medical information available, please provide a comprehensive answer about: "{query}"

        Please structure your response as follows:
        1. **Overview**: What is this condition/topic?
        2. **Key Facts**: Important characteristics, symptoms, or features
        3. **Prevention/Treatment**: Available prevention methods or treatments (if applicable)
        4. **When to Seek Help**: When to consult healthcare professionals
        5. **Additional Notes**: Any other important information

        Use bullet points where appropriate and ensure the response is medically accurate.
        Always emphasize consulting healthcare professionals for specific medical advice.
        
        Question: {query}
        """
        
        # Query using LlamaIndex query engine with enhanced prompt
        response = llamaindex_engine.query(enhanced_query)
        
        # Add medical disclaimer to LlamaIndex responses
        enhanced_response = f"{str(response)}\n\nâš ï¸ **Important:** Always consult healthcare professionals for specific medical advice."
        
        return enhanced_response
        
    except Exception as e:
        st.error(f"LlamaIndex search error: {e}")
        return None

def search_documents(vector_store, query: str, max_results: int = 10) -> List[Dict]:
    """Search for medical documents"""
    if not vector_store:
        return []
    
    # Parse query into keywords
    query_words = re.findall(r'\b\w+\b', query.lower())
    query_words = [w for w in query_words if len(w) > 2]
    
    if not query_words:
        return []
    
    # Build simple keyword index if not exists
    if not hasattr(vector_store, 'keyword_index'):
        keyword_index = {}
        for i, text in enumerate(vector_store.texts):
            words = re.findall(r'\b\w+\b', text.lower())
            for word in words:
                if len(word) > 2:
                    if word not in keyword_index:
                        keyword_index[word] = []
                    keyword_index[word].append(i)
        vector_store.keyword_index = keyword_index
    
    # Find matching documents
    doc_scores = {}
    for word in query_words:
        if word in vector_store.keyword_index:
            for doc_idx in vector_store.keyword_index[word]:
                if doc_idx not in doc_scores:
                    doc_scores[doc_idx] = 0
                
                text = vector_store.texts[doc_idx].lower()
                word_count = text.count(word)
                
                # Boost score for title/heading matches
                if word in text[:100]:
                    word_count *= 2
                
                doc_scores[doc_idx] += word_count
    
    # Sort by relevance
    sorted_docs = sorted(doc_scores.items(), key=lambda x: x[1], reverse=True)
    
    # Format results
    results = []
    for doc_idx, score in sorted_docs[:max_results]:
        result = {
            'text': vector_store.texts[doc_idx],
            'metadata': vector_store.metadata[doc_idx],
            'relevance_score': score,
            'source': vector_store.metadata[doc_idx].get('source', 'Unknown'),
            'index': doc_idx
        }
        results.append(result)
    
    return results

def get_openai_summary(query: str, results: List[Dict]) -> Optional[str]:
    """Generate AI summary using OpenAI"""
    try:
        from openai import OpenAI
        
        api_key = os.getenv('OPENAI_API_KEY')
        if not api_key or api_key == 'your-api-key-here':
            return None
        
        client = OpenAI(api_key=api_key)
        
        # Prepare context from top results
        context_parts = []
        for result in results[:3]:
            source = result['source']
            text = result['text'][:800]  # Increased text length for better context
            context_parts.append(f"Source: {source}\n{text}")
        
        context = "\n\n---\n\n".join(context_parts)
        
        # Create prompt
        prompt = f"""Based on the following medical information, provide a comprehensive answer about: "{query}"

Medical Information:
{context}

Please provide:
1. **Overview**: What is this condition/topic?
2. **Key Facts**: Important characteristics, symptoms, or features
3. **Prevention/Treatment**: Available prevention methods or treatments (if applicable)
4. **When to Seek Help**: When to consult healthcare professionals
5. **Additional Notes**: Any other important information

Format your response with clear headings and bullet points where appropriate.
Always emphasize consulting healthcare professionals for specific medical advice."""

        response = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[
                {"role": "system", "content": "You are a helpful medical AI assistant. Provide accurate, evidence-based information while always recommending professional medical consultation. Format your response clearly with headings and bullet points."},
                {"role": "user", "content": prompt}
            ],
            max_tokens=1200,
            temperature=0.1
        )
        
        return response.choices[0].message.content
        
    except Exception as e:
        st.error(f"OpenAI Error: {e}")
        return None

def get_keyword_summary(query: str, results: List[Dict]) -> str:
    """Generate simple summary without OpenAI"""
    if not results:
        return "No relevant information found in the medical database."
    
    sources = set(result['source'] for result in results)
    
    summary = f"**Found {len(results)} relevant documents** from {', '.join(sources)}.\n\n"
    
    # Extract key information from top results
    key_points = []
    for result in results[:5]:  # Use top 5 results
        text = result['text']
        # Extract first sentence or key information
        sentences = text.split('. ')
        if sentences:
            key_point = sentences[0][:250]
            if not key_point.endswith('.'):
                key_point += "..."
            key_points.append(f"â€¢ **{result['source']}**: {key_point}")
    
    if key_points:
        summary += "**Key Information:**\n" + "\n\n".join(key_points)
    
    summary += "\n\nâš ï¸ **Important:** Always consult healthcare professionals for specific medical advice."
    
    return summary

def is_medical_query(query: str) -> bool:
    """Check if the query is medical-related and appropriate for MediAid AI"""
    query_lower = query.lower()
    
    # Medical keywords that indicate legitimate medical queries
    medical_keywords = [
        # Medical conditions
        'symptom', 'symptoms', 'disease', 'condition', 'syndrome', 'disorder', 'infection',
        'diabetes', 'hypertension', 'cancer', 'heart', 'kidney', 'liver', 'lung', 'brain',
        'asthma', 'allergy', 'allergies', 'pneumonia', 'flu', 'fever', 'cough', 'pain',
        'headache', 'migraine', 'depression', 'anxiety', 'stroke', 'blood pressure',
        
        # Medical terms
        'treatment', 'therapy', 'medication', 'medicine', 'drug', 'prescription',
        'surgery', 'operation', 'procedure', 'diagnosis', 'test', 'screening',
        'vaccine', 'vaccination', 'immunization', 'prevention', 'cure',
        
        # Body parts and systems
        'chest', 'abdomen', 'stomach', 'throat', 'eye', 'ear', 'nose', 'mouth',
        'skin', 'bone', 'muscle', 'joint', 'blood', 'urine', 'bowel', 'bladder',
        
        # Medical professionals and settings
        'doctor', 'physician', 'nurse', 'hospital', 'clinic', 'emergency',
        'medical', 'health', 'healthcare', 'patient', 'diagnosis', 'prognosis',
        
        # Pregnancy and child health
        'pregnant', 'pregnancy', 'baby', 'infant', 'child', 'pediatric',
        'maternal', 'fetal', 'prenatal', 'postnatal',
        
        # Common medical questions
        'what is', 'how to treat', 'side effects', 'safe during', 'contraindication',
        'interaction', 'dosage', 'dose', 'risk', 'complication'
    ]
    
    # Non-medical topics that should be blocked
    non_medical_keywords = [
        # Technology
        'computer', 'software', 'programming', 'code', 'coding', 'website', 'app',
        'internet', 'wifi', 'smartphone', 'iphone', 'android', 'windows', 'mac',
        
        # Entertainment
        'movie', 'film', 'tv show', 'music', 'song', 'game', 'gaming', 'sport',
        'football', 'basketball', 'soccer', 'celebrity', 'actor', 'actress',
        
        # Business/Finance
        'business', 'investment', 'stock', 'money', 'bank', 'loan', 'credit',
        'insurance', 'tax', 'salary', 'job', 'career', 'resume',
        
        # Education (non-medical)
        'homework', 'essay', 'assignment', 'math', 'physics', 'chemistry', 'history',
        'geography', 'literature', 'university', 'college', 'degree',
        
        # Politics/Social
        'politics', 'government', 'election', 'president', 'vote', 'law', 'legal',
        'court', 'judge', 'lawyer', 'attorney',
        
        # Travel/Food (non-medical)
        'vacation', 'travel', 'hotel', 'restaurant', 'recipe', 'cooking', 'food',
        'cuisine', 'drink', 'alcohol', 'wine', 'beer',
        
        # Personal/Relationships
        'relationship', 'dating', 'love', 'marriage', 'divorce', 'family',
        'friend', 'friendship', 'social media', 'facebook', 'twitter', 'instagram'
    ]
    
    # Check for obvious non-medical content
    non_medical_count = sum(1 for keyword in non_medical_keywords if keyword in query_lower)
    medical_count = sum(1 for keyword in medical_keywords if keyword in query_lower)
    
    # If query contains significantly more non-medical terms, likely not medical
    if non_medical_count > medical_count and non_medical_count >= 2:
        return False
    
    # Check for explicit non-medical patterns
    non_medical_patterns = [
        'how to make', 'recipe for', 'best restaurant', 'movie recommendation',
        'song lyrics', 'game walkthrough', 'investment advice', 'stock price',
        'political opinion', 'latest news', 'weather forecast', 'sports score',
        'programming tutorial', 'code example', 'homework help', 'essay writing'
    ]
    
    if any(pattern in query_lower for pattern in non_medical_patterns):
        return False
    
    # If query contains medical keywords or medical question patterns, allow it
    medical_question_patterns = [
        'what is', 'what are', 'how to treat', 'how to prevent', 'symptoms of',
        'causes of', 'treatment for', 'medication for', 'safe during', 'side effects'
    ]
    
    has_medical_pattern = any(pattern in query_lower for pattern in medical_question_patterns)
    has_medical_keyword = medical_count > 0
    
    # Allow if it has medical context or seems like a health question
    if has_medical_keyword or has_medical_pattern:
        return True
    
    # For very short queries, be more lenient (might be medical abbreviations)
    if len(query.split()) <= 3:
        # But block obvious non-medical short phrases
        obvious_non_medical = ['pizza', 'movie', 'game', 'music', 'weather', 'news', 'politics', 'programming', 'tutorial']
        if any(term in query_lower for term in obvious_non_medical):
            return False
        return True
    
    # Default: if we can't clearly identify it as medical, be cautious
    return False

def is_complex_query(query: str) -> bool:
    """Determine if a query requires task decomposition"""
    complexity_indicators = [
        'multiple conditions', 'and', 'with', 'also have', 'plus', 'additionally',
        'interactions', 'safe during', 'pregnant', 'elderly', 'child',
        'taking medication', 'side effects', 'contraindications'
    ]
    
    query_lower = query.lower()
    # Count conditions mentioned
    medical_conditions = ['diabetes', 'hypertension', 'heart', 'kidney', 'liver', 'pregnancy', 'cancer', 'asthma']
    condition_count = len([word for word in medical_conditions if word in query_lower])
    
    # Check for complexity indicators
    has_complexity_words = any(indicator in query_lower for indicator in complexity_indicators)
    
    # Consider complex if multiple conditions OR complexity indicators present
    return condition_count > 1 or has_complexity_words or len(query.split()) > 15

def decompose_medical_query(query: str, client) -> Optional[Dict]:
    """Break down complex medical queries into sub-tasks"""
    try:
        decomposition_prompt = f"""
        Analyze this medical query and break it down into logical sub-tasks: "{query}"
        
        Return a JSON structure with:
        {{
            "main_topic": "Brief description of the main question",
            "conditions": ["condition1", "condition2"],
            "sub_questions": ["specific question 1", "specific question 2"],
            "safety_considerations": ["safety aspect 1", "safety aspect 2"],
            "complexity_level": "low/medium/high"
        }}
        
        Only return valid JSON, no other text.
        """
        
        response = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[{"role": "user", "content": decomposition_prompt}],
            max_tokens=400,
            temperature=0.1
        )
        
        return json.loads(response.choices[0].message.content)
    except Exception as e:
        st.error(f"Query decomposition failed: {e}")
        return None

def execute_complex_search(decomposition: Dict, vector_store) -> Dict:
    """Execute searches for each sub-task of a complex query"""
    results = {
        "condition_searches": {},
        "safety_searches": {},
        "question_searches": {}
    }
    
    # Search for each condition
    for condition in decomposition.get("conditions", []):
        if condition.strip():
            condition_results = search_documents(vector_store, f"{condition} symptoms treatment management", max_results=3)
            results["condition_searches"][condition] = condition_results
    
    # Search for safety considerations
    for safety in decomposition.get("safety_considerations", []):
        if safety.strip():
            safety_results = search_documents(vector_store, safety, max_results=2)
            results["safety_searches"][safety] = safety_results
    
    # Search for specific sub-questions
    for question in decomposition.get("sub_questions", []):
        if question.strip():
            question_results = search_documents(vector_store, question, max_results=3)
            results["question_searches"][question] = question_results
    
    return results

def synthesize_complex_response(user_message: str, decomposition: Dict, complex_results: Dict, chat_history: List[Dict], client) -> str:
    """Synthesize comprehensive response from multiple search results"""
    try:
        # Format all search results
        formatted_results = "RESEARCH FINDINGS:\n\n"
        
        # Add condition information
        if complex_results["condition_searches"]:
            formatted_results += "CONDITIONS RESEARCHED:\n"
            for condition, results in complex_results["condition_searches"].items():
                formatted_results += f"\n{condition.upper()}:\n"
                for i, result in enumerate(results[:2], 1):
                    formatted_results += f"{i}. {result['text'][:200]}...\n"
        
        # Add safety information
        if complex_results["safety_searches"]:
            formatted_results += "\nSAFETY CONSIDERATIONS:\n"
            for safety, results in complex_results["safety_searches"].items():
                formatted_results += f"\n{safety}:\n"
                for i, result in enumerate(results[:1], 1):
                    formatted_results += f"{i}. {result['text'][:200]}...\n"
        
        # Add specific question answers
        if complex_results["question_searches"]:
            formatted_results += "\nSPECIFIC QUESTIONS:\n"
            for question, results in complex_results["question_searches"].items():
                formatted_results += f"\nQ: {question}\n"
                if results:
                    formatted_results += f"A: {results[0]['text'][:200]}...\n"
        
        # Build conversation context
        conversation_context = ""
        if chat_history:
            recent_messages = chat_history[-3:]  # Last 3 exchanges
            for msg in recent_messages:
                conversation_context += f"User: {msg['user']}\nAssistant: {msg['assistant'][:100]}...\n\n"
        
        synthesis_prompt = f"""
        You are MediAid AI providing comprehensive medical information analysis.
        
        ORIGINAL COMPLEX QUERY: "{user_message}"
        
        CONVERSATION HISTORY:
        {conversation_context}
        
        DECOMPOSED RESEARCH:
        Main Topic: {decomposition.get('main_topic', 'N/A')}
        Conditions: {', '.join(decomposition.get('conditions', []))}
        Complexity: {decomposition.get('complexity_level', 'medium')}
        
        {formatted_results}
        
        Provide a comprehensive, structured response that:
        
        1. **OVERVIEW**: Summarize the main medical question
        2. **CONDITION ANALYSIS**: Address each condition mentioned with key facts
        3. **SAFETY CONSIDERATIONS**: Highlight important safety information, interactions, contraindications
        4. **SPECIFIC RECOMMENDATIONS**: Answer each sub-question clearly
        5. **ACTION PLAN**: Provide clear next steps
        6. **MEDICAL DISCLAIMER**: Emphasize professional consultation
        
        Format with clear headings, bullet points, and easy-to-understand language.
        Be thorough but concise. Focus on evidence-based information.
        """
        
        response = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[{"role": "user", "content": synthesis_prompt}],
            max_tokens=1000,
            temperature=0.2
        )
        
        return response.choices[0].message.content
        
    except Exception as e:
        st.error(f"Response synthesis failed: {e}")
        return "I apologize, but I encountered an error while analyzing your complex query. Please try rephrasing your question or ask about specific conditions separately."

def get_conversational_response(user_message: str, chat_history: List[Dict], results: List[Dict], document_context: str = "") -> str:
    """Generate conversational response with context from chat history and search results"""
    try:
        load_dotenv()
        api_key = os.getenv('OPENAI_API_KEY')
        
        if not api_key or api_key == 'your-api-key-here':
            return None
            
        from openai import OpenAI
        client = OpenAI(api_key=api_key)
        
        # Build context from search results
        context_parts = []
        for result in results[:3]:
            source = result['source']
            text = result['text'][:600]
            context_parts.append(f"Source: {source}\n{text}")
        
        context = "\n\n---\n\n".join(context_parts)
        
        # Build conversation history
        conversation_context = ""
        if chat_history:
            recent_messages = chat_history[-6:]  # Last 6 messages for context
            for msg in recent_messages:
                conversation_context += f"User: {msg['user']}\nAssistant: {msg['assistant']}\n\n"
        
        # Create conversational prompt
        prompt = f"""You are MediAid AI, a helpful medical information assistant. You provide accurate, easy-to-understand medical information based on CDC and WHO data.

Previous conversation:
{conversation_context}

Current medical information available:
{context}"""

        # Add document context if available
        if document_context:
            prompt += f"""

USER'S UPLOADED MEDICAL DOCUMENT:
{document_context}

IMPORTANT: The user has uploaded a medical document (shown above). Please analyze this document and answer their question based on both the uploaded document content and the general medical information provided. Explain any medical terms or values found in their document in simple language."""

        prompt += f"""

User's current question: "{user_message}"

Please respond in a natural, conversational way. Be helpful, empathetic, and provide clear medical information. If this is a follow-up question, reference the previous conversation appropriately. {"If the user uploaded a document, focus on explaining their specific document while providing relevant general medical context." if document_context else ""} Always end with a reminder to consult healthcare professionals for personal medical advice.

Your response should be:
- Conversational and natural
- Medically accurate based on the provided sources{"and uploaded document" if document_context else ""}
- Easy to understand
- Appropriately empathetic
- Include relevant follow-up suggestions if appropriate"""

        response = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[{"role": "user", "content": prompt}],
            max_tokens=800,
            temperature=0.3
        )
        
        return response.choices[0].message.content
        
    except Exception as e:
        st.error(f"AI Response Error: {e}")
        return None

def render_navigation():
    """Render navigation sidebar"""
    st.sidebar.title("ğŸ¥ MediAid AI")
    
    # Navigation buttons
    if st.sidebar.button("ğŸ  Home", use_container_width=True, key="nav_home"):
        st.session_state.current_page = 'home'
        st.session_state.selected_disease = None
        st.rerun()
    
    if st.sidebar.button("ğŸ” Search", use_container_width=True, key="nav_search"):
        st.session_state.current_page = 'search'
        st.session_state.selected_disease = None
        st.rerun()
    
    if st.sidebar.button("ğŸ“¤ Upload & Ask", use_container_width=True, key="nav_upload"):
        st.session_state.current_page = 'upload'
        st.session_state.selected_disease = None
        st.rerun()
    
    if st.sidebar.button("ğŸ“‹ Browse Topics", use_container_width=True, key="nav_browse"):
        st.session_state.current_page = 'browse'
        st.session_state.selected_disease = None
        st.rerun()

    if st.sidebar.button("â“FAQs", use_container_width=True, key="nav_faqs"):
        st.session_state.current_page = 'faqs'
        st.session_state.selected_disease = None
        st.rerun()
    
    if st.sidebar.button("ğŸ“œ Search History", use_container_width=True, key="nav_history"):
        st.session_state.current_page = 'history'
        st.session_state.selected_disease = None
        st.rerun()
    
    # Show current disease if selected
    if st.session_state.selected_disease:
        st.sidebar.markdown("---")
        st.sidebar.markdown(f"**Current Topic:**")
        st.sidebar.info(st.session_state.selected_disease)
        
        if st.sidebar.button("â¬…ï¸ Back to Browse", use_container_width=True, key="nav_back_browse"):
            st.session_state.current_page = 'browse'
            st.session_state.selected_disease = None
            st.rerun()
    
    # Check OpenAI status
    st.sidebar.markdown("---")
    
    # Try to load environment variables again
    load_dotenv()
    api_key = os.getenv('OPENAI_API_KEY')
    
    # Show LlamaIndex status
    if 'llamaindex_engine' in st.session_state and st.session_state.llamaindex_engine:
        st.sidebar.success("ğŸ¦™ LlamaIndex: Ready")
        use_llamaindex = st.sidebar.checkbox("Use LlamaIndex for search", value=False, key="use_llamaindex")
        compare_engines = st.sidebar.checkbox("Compare both search engines", value=False, key="compare_engines")
    else:
        st.sidebar.warning("ğŸ¦™ LlamaIndex: Not Available")
        use_llamaindex = False
        compare_engines = False
    
    # Removed debug information for production
    
    if api_key and api_key != 'your-api-key-here' and len(api_key) > 20:
        st.sidebar.success("ğŸ¤– AI Summaries: Enabled")
        use_ai = st.sidebar.checkbox("Use AI-powered summaries", value=True, key="ai_summaries_diabetes")
        
        # Task Decomposition Feature
        st.sidebar.success("ğŸ§  Task Decomposition: Enabled")
        st.sidebar.info("Complex queries will be automatically analyzed and broken down into sub-tasks")
        
        # Content Guardrails
        st.sidebar.success("ğŸ”’ Content Guardrails: Active")
        st.sidebar.info("Non-medical queries are automatically blocked")
        
        # Removed test button for production
    else:
        st.sidebar.warning("ğŸ¤– AI Summaries: Disabled (API key not configured)")
        st.sidebar.warning("ğŸ§  Task Decomposition: Disabled (requires OpenAI API)")
        st.sidebar.success("ğŸ”’ Content Guardrails: Active")
        use_ai = False
    
    # Logout button at the bottom
    st.sidebar.markdown("---")
    st.sidebar.markdown(f"ğŸ‘¤ **Logged in as:** {st.session_state.username}")
    
    # Policy reminder
    st.sidebar.markdown("---")
    st.sidebar.info("ğŸ”’ **Policy**: MediAid AI is designed exclusively for medical and health-related questions. Non-medical queries will be blocked.")
    
    if st.sidebar.button("ğŸšª Logout", use_container_width=True, key="nav_logout"):
        st.session_state.authenticated = False
        st.session_state.username = ""
        st.session_state.current_page = 'home'
        st.rerun()

def render_home_page(vector_store):
    """Render the home page"""
    st.title("ğŸ¥ MediAid AI - Medical Information Search")
    st.markdown("**Your comprehensive medical information assistant powered by CDC and WHO databases**")
    
    # Show system status
    col1, col2 = st.columns(2)
    
    with col1:
        if vector_store:
            st.success(f"âœ… FAISS: Loaded {vector_store.index.ntotal} medical documents")
        else:
            st.error("âŒ FAISS: Not loaded")
    
    with col2:
        llamaindex_query_engine = st.session_state.get('llamaindex_engine', None)
        if llamaindex_query_engine:
            st.success("âœ… LlamaIndex: Ready for intelligent search")
        else:
            st.warning("âš ï¸ LlamaIndex: Not available (requires OpenAI API)")
    
    # # Quick stats
    # col1, col2, col3, col4 = st.columns(4)
    # with col1:
    #     st.metric("Total Documents", "5,489")
    # with col2:
    #     st.metric("Medical Topics", "1,068+")
    # with col3:
    #     st.metric("Data Sources", "CDC + WHO")
    # with col4:
    #     st.metric("Search Types", "FAISS + LlamaIndex")
    
    # st.markdown("---")
    
    # Features overview
    st.subheader("ğŸŒŸ Features")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("""
        ### ğŸ” Smart Search
        - **FAISS**: Fast keyword-based search
        - **ğŸ¦™ LlamaIndex**: Intelligent semantic search
        - AI-powered summaries
        - Source attribution
        
        ### ğŸ“‹ Browse Topics
        - 1,068+ medical conditions
        - Dedicated disease pages
        - Detailed information
        - CDC and WHO data
        """)
    
    with col2:
        st.markdown("""
        ### ï¿½ Safety & Compliance
        - **Content Guardrails**: Medical-only queries enforced
        - **Policy Protection**: Non-medical content blocked
        - **User Authentication**: Secure access control
        - **Search History**: Personal activity tracking
        
        ### ğŸ¯ Key Benefits
        - Evidence-based information
        - Multiple search engines
        - Mobile responsive
        - Always up-to-date
        """)
    
    # # Search engine comparison
    # if llamaindex_query_engine:
    #     st.markdown("---")
    #     st.subheader("ğŸ” Search Engine Comparison")
        
    #     comparison_col1, comparison_col2 = st.columns(2)
        
    #     with comparison_col1:
    #         st.markdown("""
    #         **ğŸƒâ€â™‚ï¸ FAISS Search:**
    #         - âš¡ Ultra-fast keyword matching
    #         - ğŸ“Š Relevance scoring
    #         - ğŸ¯ Exact term matching
    #         - ğŸ’¾ Low memory usage
    #         """)
        
    #     with comparison_col2:
    #         st.markdown("""
    #         **ğŸ¦™ LlamaIndex Search:**
    #         - ğŸ§  Understands context and meaning
    #         - ğŸ’¬ Natural language queries
    #         - ğŸ“– Comprehensive responses
    #         - ğŸ¤– AI-powered insights
    #         """)
    
    # Task Decomposition Feature Demo
    st.markdown("---")
    st.subheader("ğŸ§  Advanced Task Decomposition")
    st.markdown("**New Agentic AI Feature:** Automatically breaks down complex medical queries into manageable sub-tasks")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("""
        **ğŸ” What It Does:**
        - Analyzes complex multi-condition queries
        - Breaks them into specific research tasks
        - Searches each aspect systematically
        - Synthesizes comprehensive responses
        """)
        
        st.markdown("""
        **ğŸ¯ Perfect For:**
        - Multiple medical conditions
        - Drug interactions & safety
        - Pregnancy-related questions
        - Elderly care considerations
        """)
    
    with col2:
        st.markdown("""
        **ğŸ’¡ Example Complex Queries:**
        - *"I have diabetes and high blood pressure, am pregnant, what medications are safe?"*
        - *"Elderly patient with heart disease and kidney problems - drug interactions?"*
        - *"Child with asthma and allergies - vaccine safety considerations?"*
        """)
        
        # Demo button
        if st.button("ğŸš€ Try Complex Query Demo", key="complex_demo"):
            demo_query = "I have diabetes and high blood pressure and I'm pregnant. What medications are safe?"
            st.session_state.search_query = demo_query
            st.session_state.current_page = 'search'
            st.rerun()
    
    # Quick search
    st.markdown("---")
    st.subheader("ğŸš€ Quick Search")
    
    quick_query = st.text_input(
        "Enter a medical question:",
        placeholder="e.g., acanthamoeba symptoms, vaccine safety, food poisoning",
        key="home_search"
    )
    
    if quick_query:
        # GUARDRAIL: Check if the query is medical-related
        if not is_medical_query(quick_query):
            st.error("ğŸš« **Policy Violation**: This is a violation of our application policy. MediAid AI is designed exclusively for medical and health-related questions.")
            st.warning("**Please ask questions about medical conditions, symptoms, treatments, or health concerns.**")
        else:
            st.session_state.search_query = quick_query
            st.session_state.current_page = 'search'
            st.rerun()

def render_search_page(vector_store):
    """Render the chat-based search page"""
    st.title("ğŸ’¬ Chat with MediAid AI")
    st.markdown("Ask me anything about medical conditions, symptoms, treatments, or prevention. I can help with follow-up questions too!")
    
    # Check OpenAI status for AI responses
    load_dotenv()
    api_key = os.getenv('OPENAI_API_KEY')
    use_ai = api_key and api_key != 'your-api-key-here' and len(api_key) > 20
    
    if use_ai:
        st.sidebar.success("ğŸ¤– AI Chat: Enabled")
    else:
        st.sidebar.warning("ğŸ¤– AI Chat: Disabled (using keyword responses)")
    
    # Display chat history
    if st.session_state.chat_history:
        st.subheader("ğŸ“ Chat History")
        for i, chat in enumerate(st.session_state.chat_history):
            # User message
            with st.container():
                st.markdown(f"**ğŸ§‘ You:** {chat['user']}")
            
            # Assistant response
            with st.container():
                st.markdown(f"**ğŸ¤– MediAid AI:** {chat['assistant']}")
            
            if i < len(st.session_state.chat_history) - 1:
                st.markdown("---")
    
    # Chat input area
    st.subheader("ğŸ’¬ Ask a Question")
    
    # Use chat_input for a more natural chat experience
    user_message = st.chat_input("Type your medical question here...")
    
    # Alternative fallback input if chat_input is not available
    if not user_message:
        with st.form("chat_form", clear_on_submit=True):
            user_message = st.text_area(
                "Your message:",
                placeholder="e.g., What are the symptoms of diabetes? How can I prevent heart disease?",
                help="Ask any medical question - I can provide follow-up answers too!"
            )
            submitted = st.form_submit_button("Send ğŸ’¬")
            if not submitted:
                user_message = None
    
    if user_message:
        # GUARDRAIL: Check if the query is medical-related
        if not is_medical_query(user_message):
            st.error("ğŸš« **Policy Violation**: This is a violation of our application policy. MediAid AI is designed exclusively for medical and health-related questions. Please ask questions about medical conditions, symptoms, treatments, medications, or health concerns.")
            st.warning("**Examples of appropriate questions:**\n- What are the symptoms of diabetes?\n- How to treat high blood pressure?\n- Side effects of aspirin\n- Safe medications during pregnancy")
            return
        
        # Check if user wants to use LlamaIndex or compare both
        use_llamaindex = st.session_state.get('use_llamaindex', False)
        compare_engines = st.session_state.get('compare_engines', False)
        llamaindex_engine = st.session_state.get('llamaindex_engine', None)
        
        if compare_engines and llamaindex_engine:
            # Show both responses side by side
            st.subheader("ğŸ” Comparing Search Engines")
            
            col1, col2 = st.columns(2)
            
            with col1:
                st.markdown("### ğŸ¦™ LlamaIndex Response")
                with st.spinner("ğŸ¦™ Searching with LlamaIndex..."):
                    llamaindex_response = search_documents_llamaindex(llamaindex_engine, user_message)
                
                if llamaindex_response:
                    st.info("ğŸ¦™ LlamaIndex Results")
                    st.markdown(llamaindex_response)
                else:
                    st.error("âŒ LlamaIndex search failed")
            
            with col2:
                st.markdown("### âš¡ FAISS + AI Response")
                with st.spinner("âš¡ Searching with FAISS..."):
                    results = search_documents(vector_store, user_message, max_results=8)
                
                if results and use_ai:
                    ai_response = get_conversational_response(user_message, st.session_state.chat_history, results)
                    if ai_response:
                        st.info("âš¡ FAISS + OpenAI Results")
                        st.markdown(ai_response)
                    else:
                        st.error("âŒ AI response failed")
                elif results:
                    summary = get_keyword_summary(user_message, results)
                    st.info("âš¡ FAISS Keyword Results")
                    st.markdown(summary)
                else:
                    st.error("âŒ No results found")
            
            # Don't add to chat history for comparisons
            return
        
        elif use_llamaindex and llamaindex_engine:
            # Use LlamaIndex for search
            with st.spinner("ğŸ¦™ Searching with LlamaIndex..."):
                llamaindex_response = search_documents_llamaindex(llamaindex_engine, user_message)
            
            if llamaindex_response:
                # Add to chat history
                st.session_state.chat_history.append({
                    'user': user_message,
                    'assistant': f"**ğŸ¦™ LlamaIndex Response:**\n\n{llamaindex_response}"
                })
                
                # Save to search history
                save_search_history(st.session_state.username, user_message, llamaindex_response, "llamaindex_search")
                
                # Show response immediately
                st.success("âœ… LlamaIndex response generated!")
                with st.container():
                    st.markdown(f"**ğŸ§‘ You:** {user_message}")
                    st.info("ğŸ¦™ Response generated using LlamaIndex")
                    st.markdown(f"**ğŸ¤– MediAid AI:** {llamaindex_response}")
                
                st.rerun()
            else:
                st.error("âŒ LlamaIndex search failed. Falling back to keyword search.")
                use_llamaindex = False
        
        if not use_llamaindex or not llamaindex_engine:
            # Check if this is a complex query that needs decomposition
            is_complex = is_complex_query(user_message)
            
            if is_complex and use_ai:
                # Complex query path with task decomposition
                st.info("ğŸ§  **Complex Query Detected** - Using advanced task decomposition...")
                
                with st.spinner("ğŸ” Analyzing and decomposing your complex medical question..."):
                    from openai import OpenAI
                    client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))
                    
                    # Step 1: Decompose the query
                    decomposition = decompose_medical_query(user_message, client)
                
                if decomposition:
                    # Show decomposition to user
                    with st.expander("ğŸ§© Query Analysis", expanded=True):
                        st.markdown(f"**Main Topic:** {decomposition.get('main_topic', 'N/A')}")
                        st.markdown(f"**Conditions to Research:** {', '.join(decomposition.get('conditions', []))}")
                        st.markdown(f"**Complexity Level:** {decomposition.get('complexity_level', 'medium').title()}")
                        if decomposition.get('sub_questions'):
                            st.markdown("**Sub-questions to Address:**")
                            for q in decomposition['sub_questions']:
                                st.write(f"â€¢ {q}")
                    
                    # Step 2: Execute multi-faceted search
                    with st.spinner("ğŸ” Conducting comprehensive research across multiple medical areas..."):
                        complex_results = execute_complex_search(decomposition, vector_store)
                    
                    # Count total sources found
                    total_sources = sum(len(results) for category in complex_results.values() for results in category.values())
                    st.success(f"âœ… Found {total_sources} relevant medical sources across multiple research areas")
                    
                    # Step 3: Synthesize comprehensive response
                    with st.spinner("ğŸ¤– Synthesizing comprehensive response from multiple research streams..."):
                        complex_response = synthesize_complex_response(user_message, decomposition, complex_results, st.session_state.chat_history, client)
                    
                    if complex_response:
                        # Add to chat history
                        st.session_state.chat_history.append({
                            'user': user_message,
                            'assistant': complex_response
                        })
                        
                        # Save to search history with special type
                        save_search_history(st.session_state.username, user_message, complex_response, "complex_analysis")
                        
                        # Show response
                        st.success("âœ… Comprehensive analysis complete!")
                        with st.container():
                            st.markdown(f"**ğŸ§‘ You:** {user_message}")
                            st.info("ğŸ§  Response generated using Advanced Task Decomposition")
                            st.markdown(f"**ğŸ¤– MediAid AI:** {complex_response}")
                        
                        # Show detailed sources by category
                        with st.expander("ğŸ“š Research Sources by Category"):
                            if complex_results["condition_searches"]:
                                st.markdown("### ğŸ©º Condition Research")
                                for condition, results in complex_results["condition_searches"].items():
                                    st.markdown(f"**{condition}:**")
                                    for i, result in enumerate(results[:2], 1):
                                        st.write(f"{i}. {result['source']} - {result['text'][:150]}...")
                            
                            if complex_results["safety_searches"]:
                                st.markdown("### âš ï¸ Safety Research")
                                for safety, results in complex_results["safety_searches"].items():
                                    st.markdown(f"**{safety}:**")
                                    for i, result in enumerate(results[:1], 1):
                                        st.write(f"{i}. {result['source']} - {result['text'][:150]}...")
                        
                        st.rerun()
                    else:
                        st.error("âŒ Complex analysis failed. Falling back to standard search.")
                        # Fall back to standard search
                        results = search_documents(vector_store, user_message, max_results=8)
                else:
                    st.warning("âš ï¸ Could not decompose query. Using standard search.")
                    # Fall back to standard search
                    results = search_documents(vector_store, user_message, max_results=8)
            else:
                # Standard search path
                with st.spinner("Searching medical database and generating response..."):
                    results = search_documents(vector_store, user_message, max_results=8)
        
        if results:
            # Generate conversational response
            if use_ai:
                ai_response = get_conversational_response(user_message, st.session_state.chat_history, results)
                
                if ai_response:
                    # Add to chat history
                    st.session_state.chat_history.append({
                        'user': user_message,
                        'assistant': ai_response
                    })
                    
                    # Save to search history
                    save_search_history(st.session_state.username, user_message, ai_response, "ai_search")
                    
                    # Show latest response immediately
                    st.success("Response generated!")
                    with st.container():
                        st.markdown(f"**ğŸ§‘ You:** {user_message}")
                        st.markdown(f"**ğŸ¤– MediAid AI:** {ai_response}")
                    
                    # Add sources section
                    with st.expander("ğŸ“š Sources Used"):
                        for i, result in enumerate(results[:3], 1):
                            st.markdown(f"**{i}. {result['source']}** (Relevance: {result['relevance_score']})")
                            st.markdown(result['text'][:300] + "...")
                            if i < 3:
                                st.markdown("---")
                    
                    st.rerun()
                else:
                    st.error("AI response unavailable. Please try again.")
            else:
                # Fallback to keyword summary
                summary = get_keyword_summary(user_message, results)
                
                # Add to chat history
                st.session_state.chat_history.append({
                    'user': user_message,
                    'assistant': summary
                })
                
                # Save to search history
                save_search_history(st.session_state.username, user_message, summary, "keyword_search")
                
                st.success("Response generated!")
                with st.container():
                    st.markdown(f"**ğŸ§‘ You:** {user_message}")
                    st.markdown(f"**ğŸ¤– MediAid AI:** {summary}")
                
                st.rerun()
        else:
            st.warning("I couldn't find relevant information for your question. Please try rephrasing or ask about common medical topics.")
    
    # Clear chat button
    if st.session_state.chat_history:
        st.markdown("---")
        col1, col2 = st.columns([1, 4])
        with col1:
            if st.button("ğŸ—‘ï¸ Clear Chat", key="clear_chat_btn"):
                st.session_state.chat_history = []
                st.session_state.chat_context = []
                st.rerun()
        with col2:
            st.markdown(f"*{len(st.session_state.chat_history)} messages in chat*")

def render_upload_page(vector_store):
    """Render the Upload & Ask page for medical document analysis"""
    st.title("ğŸ“„ Upload & Ask")
    st.markdown("Upload medical reports, prescriptions, or lab results and ask questions about them!")
    
    # Import OCR utilities
    try:
        from utils.ocr_utils import create_ocr_interface
        ocr_available = True
    except ImportError:
        ocr_available = False
    
    # Check OpenAI status for AI responses
    load_dotenv()
    api_key = os.getenv('OPENAI_API_KEY')
    use_ai = api_key and api_key != 'your-api-key-here' and len(api_key) > 20
    
    # Get LlamaIndex engine from session state
    llamaindex_engine = st.session_state.get('llamaindex_engine', None)
    
    if use_ai:
        st.sidebar.success("ğŸ¤– AI Analysis: Enabled")
    else:
        st.sidebar.warning("ğŸ¤– AI Analysis: Disabled (using keyword responses)")
    
    # LlamaIndex status for upload page
    if llamaindex_engine:
        st.sidebar.success("ğŸ¦™ LlamaIndex: Available for document analysis")
        use_llamaindex_upload = st.sidebar.checkbox("Use LlamaIndex for document analysis", value=False, key="use_llamaindex_upload")
        st.sidebar.info("LlamaIndex provides enhanced contextual understanding of your documents")
    else:
        st.sidebar.warning("ğŸ¦™ LlamaIndex: Not available")
        use_llamaindex_upload = False
    
    # OCR Status indicator
    if ocr_available:
        st.sidebar.success("ğŸ“„ Document Reading: Enabled")
    else:
        st.sidebar.error("ğŸ“„ Document Reading: Disabled")
        st.error("âŒ Document upload feature requires additional packages.")
        st.info("To enable document reading, install: `pip install easyocr opencv-python Pillow`")
        st.code("pip install easyocr opencv-python Pillow", language="bash")
        return
    
    # Document upload section
    st.subheader("ğŸ“¤ Upload Medical Document")
    
    # Create OCR interface
    extracted_text = create_ocr_interface()
    
    if extracted_text:
        # Store extracted text in session state
        st.session_state.document_context = extracted_text
        st.session_state.upload_page_document = True
        
        st.success("âœ… Document processed successfully!")
        
        # Document analysis section
        if 'document_analysis' in st.session_state and st.session_state.document_analysis:
            st.subheader("ğŸ” Quick Analysis")
            analysis = st.session_state.document_analysis
            
            cols = st.columns(2)
            with cols[0]:
                if 'medications' in analysis:
                    st.markdown("**ğŸ’Š Medications Found:**")
                    for med in analysis['medications'][:3]:
                        st.write(f"â€¢ {med}")
                
                if 'diagnoses' in analysis:
                    st.markdown("**ğŸ©º Diagnoses Found:**")
                    for diag in analysis['diagnoses'][:3]:
                        st.write(f"â€¢ {diag}")
            
            with cols[1]:
                if 'lab_values' in analysis:
                    st.markdown("**ğŸ“Š Lab Values Found:**")
                    for lab in analysis['lab_values'][:3]:
                        st.write(f"â€¢ {lab}")
                
                if 'vitals' in analysis:
                    st.markdown("**â¤ï¸ Vitals Found:**")
                    for vital in analysis['vitals'][:3]:
                        st.write(f"â€¢ {vital}")
        
        # Suggested questions
        st.subheader("ğŸ’¡ Suggested Questions")
        
        # Show which analysis method will be used
        if use_llamaindex_upload and llamaindex_engine:
            st.info("ğŸ¦™ **LlamaIndex Enhanced Analysis** - Click any question below for advanced AI analysis of your document:")
        else:
            st.info("âš¡ **FAISS + AI Analysis** - Click any question below to analyze your document:")
        
        col1, col2 = st.columns(2)
        with col1:
            if st.button("ğŸ“‹ What medications are mentioned?", key="upload_med_question"):
                st.session_state.suggested_question = "What medications are mentioned in this document?"
            if st.button("ğŸ©º What are the diagnoses?", key="upload_diag_question"):
                st.session_state.suggested_question = "What diagnoses or medical conditions are mentioned in this document?"
            if st.button("ğŸ“Š What are the lab values?", key="upload_lab_question"):
                st.session_state.suggested_question = "What are the lab values or test results mentioned in this document?"
        
        with col2:
            if st.button("âš•ï¸ Explain this report in simple terms", key="upload_explain_question"):
                st.session_state.suggested_question = "Can you explain what this medical report means in simple terms?"
            if st.button("âš ï¸ Are there any concerning values?", key="upload_concern_question"):
                st.session_state.suggested_question = "Are there any concerning values or findings in this document that I should be aware of?"
            if st.button("ğŸ’¡ What should I ask my doctor?", key="upload_doctor_question"):
                st.session_state.suggested_question = "Based on this document, what questions should I ask my doctor during my next visit?"
    
    # Chat section for document-specific questions
    if 'document_context' in st.session_state and st.session_state.document_context and 'upload_page_document' in st.session_state:
        st.markdown("---")
        st.subheader("ğŸ’¬ Ask Questions About Your Document")
        
        # Display uploaded document info
        with st.expander("ğŸ“„ View Uploaded Document Text"):
            st.text_area("Extracted Text:", st.session_state.document_context, height=200, disabled=True)
        
        # Clear document button
        col1, col2 = st.columns([1, 4])
        with col1:
            if st.button("ğŸ—‘ï¸ Clear Document", key="clear_upload_doc"):
                if 'document_context' in st.session_state:
                    del st.session_state.document_context
                if 'document_analysis' in st.session_state:
                    del st.session_state.document_analysis
                if 'upload_page_document' in st.session_state:
                    del st.session_state.upload_page_document
                if 'upload_chat_history' in st.session_state:
                    del st.session_state.upload_chat_history
                st.success("Document cleared!")
                st.rerun()
        
        # Initialize upload-specific chat history
        if 'upload_chat_history' not in st.session_state:
            st.session_state.upload_chat_history = []
        
        # Display chat history for this document
        if st.session_state.upload_chat_history:
            st.markdown("**ğŸ“ Questions & Answers:**")
            for i, chat in enumerate(st.session_state.upload_chat_history):
                with st.container():
                    st.markdown(f"**ğŸ§‘ You:** {chat['user']}")
                    st.markdown(f"**ğŸ¤– MediAid AI:** {chat['assistant']}")
                if i < len(st.session_state.upload_chat_history) - 1:
                    st.markdown("---")
        
        # Chat input
        user_message = None
        
        # Check for suggested questions
        if 'suggested_question' in st.session_state:
            user_message = st.session_state.suggested_question
            del st.session_state.suggested_question
        
        # Chat input
        if not user_message:
            user_message = st.chat_input("Ask a question about your uploaded document...")
        
        # Alternative input
        if not user_message:
            with st.form("upload_chat_form", clear_on_submit=True):
                user_message = st.text_area(
                    "Your question about the document:",
                    placeholder="e.g., What does this lab result mean? Are my medication dosages normal?",
                    help="Ask specific questions about your uploaded medical document"
                )
                submitted = st.form_submit_button("Ask ğŸ’¬")
                if not submitted:
                    user_message = None
        
        if user_message:
            # GUARDRAIL: Check if the query is medical-related
            if not is_medical_query(user_message):
                st.error("ğŸš« **Policy Violation**: This is a violation of our application policy. MediAid AI is designed exclusively for medical and health-related questions. Please ask questions about your medical document.")
                st.warning("**Examples of appropriate questions about your document:**\n- What medications are mentioned?\n- What are the lab results?\n- Explain this medical report\n- Are there any concerning values?")
                return
            
            # Add debug information
            st.info(f"ğŸ” Processing your question: {user_message}")
            
            # Prepare enhanced query with document context
            document_context = st.session_state.document_context
            enhanced_query = f"""
            User uploaded a medical document with the following content:
            
            DOCUMENT CONTENT:
            {document_context}
            
            USER QUESTION ABOUT THE DOCUMENT:
            {user_message}
            
            Please answer the user's question based on the uploaded document content and provide additional relevant medical information if helpful.
            """
            
            # Choose search method based on user preference
            if use_llamaindex_upload and llamaindex_engine:
                # Use LlamaIndex for enhanced document analysis
                st.info("ğŸ¦™ Using LlamaIndex for enhanced document analysis...")
                
                with st.spinner("ğŸ§  Analyzing document with advanced AI..."):
                    try:
                        # Create a more detailed prompt for LlamaIndex with document context
                        llamaindex_prompt = f"""
                        Based on the uploaded medical document and medical knowledge base, please provide a comprehensive analysis.
                        
                        UPLOADED DOCUMENT CONTENT:
                        {document_context}
                        
                        USER'S QUESTION:
                        {user_message}
                        
                        Please provide:
                        1. **Document Analysis**: What the uploaded document shows
                        2. **Answer to Question**: Direct response to the user's question
                        3. **Medical Context**: Additional relevant medical information
                        4. **Recommendations**: Next steps or considerations
                        
                        Base your response on both the document content and medical knowledge.
                        """
                        
                        llamaindex_response = search_documents_llamaindex(llamaindex_engine, llamaindex_prompt)
                        
                        if llamaindex_response:
                            # Add to upload-specific chat history
                            st.session_state.upload_chat_history.append({
                                'user': user_message,
                                'assistant': llamaindex_response
                            })
                            
                            # Save to search history
                            save_search_history(st.session_state.username, user_message, llamaindex_response, "llamaindex_document_analysis")
                            
                            # Show response immediately
                            st.success("âœ… LlamaIndex analysis complete!")
                            with st.container():
                                st.markdown(f"**ğŸ§‘ You:** {user_message}")
                                st.info("ğŸ¦™ Response generated using LlamaIndex with your uploaded document")
                                st.markdown(f"**ğŸ¤– MediAid AI:** {llamaindex_response}")
                            
                            # Show document context used
                            with st.expander("ğŸ“„ Document Context Used"):
                                st.markdown("**Your Uploaded Document:**")
                                st.text_area("Document Content:", document_context, height=200, disabled=True)
                            
                            return
                        else:
                            st.warning("âš ï¸ LlamaIndex analysis failed. Falling back to FAISS search.")
                            use_llamaindex_upload = False
                    except Exception as e:
                        st.error(f"âŒ LlamaIndex error: {e}")
                        st.warning("âš ï¸ Falling back to FAISS search.")
                        use_llamaindex_upload = False
            
            # Standard FAISS search for document analysis
            if not use_llamaindex_upload:
                st.info("âš¡ Using FAISS search for document analysis...")
                
                # Search for relevant documents
                with st.spinner("Analyzing your document and searching medical database..."):
                    try:
                        results = search_documents(vector_store, enhanced_query, max_results=5)
                        st.success(f"âœ… Found {len(results)} relevant medical references")
                    except Exception as e:
                        st.error(f"âŒ Search error: {e}")
                        results = []
            
            if results:
                # Generate response using FAISS + AI
                if use_ai:
                    st.info("ğŸ¤– Generating AI response with FAISS search results...")
                    try:
                        ai_response = get_conversational_response(enhanced_query, st.session_state.upload_chat_history, results, document_context)
                        
                        if ai_response:
                            # Add to upload-specific chat history
                            st.session_state.upload_chat_history.append({
                                'user': user_message,
                                'assistant': ai_response
                            })
                            
                            # Save to search history
                            save_search_history(st.session_state.username, user_message, ai_response, "faiss_document_analysis")
                            
                            # Show response immediately
                            st.success("âœ… FAISS + AI analysis complete!")
                            with st.container():
                                st.markdown(f"**ğŸ§‘ You:** {user_message}")
                                st.info("âš¡ Response based on FAISS search + AI analysis of your document")
                                st.markdown(f"**ğŸ¤– MediAid AI:** {ai_response}")
                            
                            # Add sources
                            with st.expander("ğŸ“š Sources Used"):
                                st.markdown("**ğŸ“„ Your Uploaded Document**")
                                st.markdown(document_context[:500] + "..." if len(document_context) > 500 else document_context)
                                st.markdown("---")
                                
                                st.markdown("**ğŸ” Medical Database References**")
                                for i, result in enumerate(results[:3], 1):
                                    st.markdown(f"**{i}. {result['source']}** (Relevance: {result['relevance_score']})")
                                    st.markdown(result['text'][:300] + "...")
                                    if i < 3:
                                        st.markdown("---")
                        else:
                            st.error("âŒ AI analysis failed. Please try again.")
                    except Exception as e:
                        st.error(f"âŒ AI analysis error: {e}")
                        import traceback
                        st.error(f"Debug: {traceback.format_exc()}")
                else:
                    # Fallback to keyword summary
                    st.info("ğŸ” Using keyword-based analysis...")
                    try:
                        summary = get_keyword_summary(user_message, results)
                        summary = f"Based on your uploaded document and medical database:\n\n{summary}"
                        
                        st.session_state.upload_chat_history.append({
                            'user': user_message,
                            'assistant': summary
                        })
                        
                        # Save to search history
                        save_search_history(st.session_state.username, user_message, summary, "keyword_document_analysis")
                        
                        st.success("âœ… Keyword analysis complete!")
                        with st.container():
                            st.markdown(f"**ğŸ§‘ You:** {user_message}")
                            st.info("ğŸ” Response based on keyword analysis")
                            st.markdown(f"**ğŸ¤– MediAid AI:** {summary}")
                    except Exception as e:
                        st.error(f"âŒ Keyword analysis error: {e}")
            else:
                st.warning("No relevant medical information found. Please try rephrasing your question.")
                st.info("Debug: Search returned no results")
    
    else:
        # No document uploaded yet
        st.info("ğŸ‘† Upload a medical document above to start asking questions about it!")
        
        # Explain search engine options
        if llamaindex_engine:
            st.markdown("---")
            st.subheader("ğŸ” Document Analysis Options")
            
            col1, col2 = st.columns(2)
            
            with col1:
                st.markdown("""
                **âš¡ FAISS Search:**
                - Fast keyword-based analysis
                - Combines document + medical database
                - Good for specific medical terms
                - Reliable and proven approach
                """)
            
            with col2:
                st.markdown("""
                **ğŸ¦™ LlamaIndex Analysis:**
                - Advanced contextual understanding
                - Enhanced document comprehension
                - Better integration of information
                - AI-powered insights *(Recommended)*
                """)
        
        # Example section
        st.subheader("ğŸ“‹ What You Can Upload")
        
        col1, col2 = st.columns(2)
        with col1:
            st.markdown("""
            **ğŸ“„ Medical Reports:**
            â€¢ Blood test results
            â€¢ X-ray reports
            â€¢ MRI/CT scan reports
            â€¢ Pathology reports
            """)
            
            st.markdown("""
            **ğŸ’Š Prescriptions:**
            â€¢ Medication lists
            â€¢ Prescription slips
            â€¢ Pharmacy labels
            """)
        
        with col2:
            st.markdown("""
            **ğŸ©º Clinical Documents:**
            â€¢ Doctor's notes
            â€¢ Discharge summaries
            â€¢ Consultation letters
            â€¢ Medical certificates
            """)
            
            st.markdown("""
            **ğŸ“Š Lab Results:**
            â€¢ Complete blood count
            â€¢ Lipid panels
            â€¢ Glucose tests
            â€¢ Hormone levels
            """)

def render_browse_page(vector_store):
    """Render the browse topics page"""
    st.title("ğŸ“‹ Browse Medical Topics")
    
    # Get available topics
    topics = get_medical_topics(vector_store)
    
    if topics:
        st.write(f"**{len(topics)} medical topics available**")
        
        # Search topics
        col1, col2 = st.columns([3, 1])
        with col1:
            topic_search = st.text_input("ğŸ” Search topics:", placeholder="Type to filter topics...")
        with col2:
            st.metric("Available Topics", len(topics))
        
        if topic_search:
            filtered_topics = [t for t in topics if topic_search.lower() in t.lower()]
            st.info(f"Found {len(filtered_topics)} topics matching '{topic_search}'")
        else:
            filtered_topics = topics
        
        # Display topics in columns with pagination
        topics_per_page = 15
        total_pages = (len(filtered_topics) + topics_per_page - 1) // topics_per_page
        
        if total_pages > 1:
            page = st.selectbox("Select page:", range(1, total_pages + 1)) - 1
        else:
            page = 0
        
        start_idx = page * topics_per_page
        end_idx = min(start_idx + topics_per_page, len(filtered_topics))
        current_topics = filtered_topics[start_idx:end_idx]
        
        # Display topics in a grid
        cols = st.columns(3)
        for i, topic in enumerate(current_topics):
            col_idx = i % 3
            with cols[col_idx]:
                # Create a card-like display
                st.markdown(f"""
                <div style="
                    border: 1px solid #ddd; 
                    border-radius: 8px; 
                    padding: 10px; 
                    margin: 5px 0; 
                    background-color: #f9f9f9;
                    height: 100px;
                    display: flex;
                    align-items: center;
                    justify-content: center;
                    text-align: center;
                ">
                    <strong>{topic}</strong>
                </div>
                """, unsafe_allow_html=True)
                
                # Button to view topic details
                if st.button(f"ğŸ“– View Details", key=f"topic_{i}_{page}", use_container_width=True):
                    st.session_state.selected_disease = topic
                    st.session_state.current_page = 'disease_detail'
                    st.rerun()
        
        # Show pagination info
        if total_pages > 1:
            st.markdown(f"Page {page + 1} of {total_pages} | Showing {start_idx + 1}-{end_idx} of {len(filtered_topics)} topics")
    
    else:
        st.error("No topics found in the database.")

def render_disease_detail_page(vector_store):
    """Render detailed page for a specific disease"""
    disease = st.session_state.selected_disease
    
    if not disease:
        st.error("No disease selected")
        return
    
    # Header with navigation
    col1, col2 = st.columns([4, 1])
    with col1:
        st.title(f"ğŸ¦  {disease}")
    with col2:
        if st.button("â¬…ï¸ Back to Browse", use_container_width=True, key="back_to_browse_detail"):
            st.session_state.current_page = 'browse'
            st.session_state.selected_disease = None
            st.rerun()
    
    st.markdown("---")
    
    # Search for information about this disease
    with st.spinner(f"Loading comprehensive information about {disease}..."):
        results = search_documents(vector_store, disease, max_results=15)
    
    if results:
        # Stats
        col1, col2, col3 = st.columns(3)
        sources = {}
        for result in results:
            source = result['source']
            sources[source] = sources.get(source, 0) + 1
        
        with col1:
            st.metric("Total Documents", len(results))
        with col2:
            st.metric("CDC Sources", sources.get('CDC', 0))
        with col3:
            st.metric("WHO Sources", sources.get('WHO', 0))
        
        # AI Summary Section
        st.subheader("ğŸ“‹ Comprehensive Summary")
        
        api_key = os.getenv('OPENAI_API_KEY')
        use_ai = api_key and api_key != 'your-api-key-here'
        
        if use_ai:
            with st.spinner("Generating comprehensive AI summary..."):
                ai_summary = get_openai_summary(f"What is {disease}? Provide comprehensive information.", results)
            
            if ai_summary:
                st.markdown(ai_summary)
            else:
                st.warning("AI summary unavailable. Showing detailed keyword summary.")
                st.markdown(get_keyword_summary(disease, results))
        else:
            st.markdown(get_keyword_summary(disease, results))
        
        # Detailed Information Sections
        st.markdown("---")
        st.subheader("ğŸ“š Detailed Information")
        
        # Group results by source
        cdc_results = [r for r in results if r['source'] == 'CDC']
        who_results = [r for r in results if r['source'] == 'WHO']
        
        # CDC Information
        if cdc_results:
            with st.expander(f"ğŸ›ï¸ CDC Information ({len(cdc_results)} documents)", expanded=True):
                for i, result in enumerate(cdc_results[:5], 1):
                    st.markdown(f"**Document {i}** (Relevance: {result['relevance_score']})")
                    st.markdown(result['text'])
                    st.markdown("---")
        
        # WHO Information
        if who_results:
            with st.expander(f"ğŸŒ WHO Information ({len(who_results)} documents)", expanded=True):
                for i, result in enumerate(who_results[:5], 1):
                    st.markdown(f"**Document {i}** (Relevance: {result['relevance_score']})")
                    st.markdown(result['text'])
                    st.markdown("---")
        
        # Related Topics
        st.subheader("ğŸ”— Related Topics")
        related_keywords = disease.split()[:3]  # Use first 3 words as related keywords
        
        for keyword in related_keywords:
            if len(keyword) > 3:
                if st.button(f"ğŸ” Search: {keyword}", key=f"related_{keyword}"):
                    st.session_state.search_query = keyword
                    st.session_state.current_page = 'search'
                    st.rerun()
    
    else:
        st.error(f"No information found for {disease}")
        st.info("This topic might not be available in our current database. Try searching with different keywords.")

def render_examples_page(vector_store):
    """Render the examples page"""
    st.title("ğŸ’¡ Quick Examples")
    st.markdown("Click any example below to see how the search works:")
    
    examples = [
        ("Acanthamoeba Eye Infections", "acanthamoeba symptoms treatment", "Learn about this rare but serious eye infection caused by a microscopic organism"),
        ("Child Vaccination Safety", "vaccine children safety side effects", "Find information about vaccine safety, schedules, and side effects for children"),
        ("Tick-Borne Disease Prevention", "tick bite prevention lyme disease", "Discover how to prevent tick bites and tick-borne diseases like Lyme disease"),
        ("Food Poisoning Information", "food poisoning symptoms treatment prevention", "Learn about foodborne illnesses, symptoms, and prevention methods"),
        ("Refugee Health Screening", "refugee health screening requirements", "Information about health requirements and screening for refugees"),
        ("Harmful Algal Blooms", "harmful algal blooms health effects", "Learn about the health effects of harmful algal blooms in water"),
        ("Travel Health Advice", "travel health vaccination requirements", "Find travel health recommendations and vaccination requirements"),
        ("Workplace Safety Guidelines", "workplace health safety guidelines", "Occupational health and safety information for various industries")
    ]
    
    for i, (title, query, description) in enumerate(examples):
        with st.expander(f"ğŸ” {title}", expanded=False):
            st.markdown(f"**Description:** {description}")
            st.markdown(f"**Search Query:** `{query}`")
            
            col1, col2 = st.columns([1, 1])
            with col1:
                if st.button(f"ğŸš€ Try This Search", key=f"example_search_{i}"):
                    st.session_state.search_query = query
                    st.session_state.current_page = 'search'
                    st.rerun()
            
            with col2:
                if st.button(f"ğŸ“‹ Browse Related Topics", key=f"example_browse_{i}"):
                    st.session_state.current_page = 'browse'
                    st.rerun()

def render_history_page():
    """Render the search history page"""
    st.title("ğŸ“œ Search History")
    st.markdown(f"**Search history for user:** {st.session_state.username}")
    
    # Load user history
    user_history = load_user_history(st.session_state.username)
    
    if not user_history:
        st.info("ğŸ” No search history found. Start searching to build your history!")
        st.markdown("### ğŸš€ Quick Actions")
        col1, col2 = st.columns(2)
        with col1:
            if st.button("ğŸ” Start Searching", use_container_width=True):
                st.session_state.current_page = 'search'
                st.rerun()
        with col2:
            if st.button("ğŸ“¤ Upload & Ask", use_container_width=True):
                st.session_state.current_page = 'upload'
                st.rerun()
        return
    
    # Filter options
    st.subheader("ğŸ” Filter History")
    col1, col2 = st.columns(2)
    
    with col1:
        search_type_filter = st.selectbox(
            "Filter by search type:",
            ["All", "ai_search", "document_analysis", "llamaindex_search", "keyword_search", "document_keyword_analysis"]
        )
    
    with col2:
        entries_to_show = st.selectbox("Show entries:", [10, 25, 50, 100], index=1)
    
    # Filter history
    filtered_history = user_history
    if search_type_filter != "All":
        filtered_history = [entry for entry in user_history if entry.get('search_type') == search_type_filter]
    
    # Sort by timestamp (newest first)
    filtered_history = sorted(filtered_history, key=lambda x: x.get('timestamp', ''), reverse=True)
    filtered_history = filtered_history[:entries_to_show]
    
    # Search within history
    search_term = st.text_input("ğŸ” Search within your history:", placeholder="Enter keywords to find specific searches...")
    if search_term:
        search_term_lower = search_term.lower()
        filtered_history = [
            entry for entry in filtered_history 
            if search_term_lower in entry.get('query', '').lower() or search_term_lower in entry.get('response', '').lower()
        ]
        st.info(f"Found {len(filtered_history)} entries matching '{search_term}'")
    
    # Display history
    st.subheader(f"ğŸ“ Recent Searches ({len(filtered_history)} entries)")
    
    if not filtered_history:
        st.warning("No entries match your current filters.")
        return
    
    for i, entry in enumerate(filtered_history):
        timestamp = entry.get('timestamp', 'Unknown time')
        query = entry.get('query', 'No query')
        response = entry.get('response', 'No response')
        search_type = entry.get('search_type', 'unknown')
        full_length = entry.get('full_response_length', len(response))
        
        # Format timestamp
        try:
            dt = datetime.fromisoformat(timestamp)
            formatted_time = dt.strftime("%Y-%m-%d %H:%M:%S")
        except:
            formatted_time = timestamp
        
        # Search type emoji
        type_emoji = {
            'ai_search': 'ğŸ¤–',
            'document_analysis': 'ğŸ“„',
            'llamaindex_search': 'ğŸ¦™',
            'keyword_search': 'ğŸ”',
            'document_keyword_analysis': 'ğŸ“‹'
        }
        
        with st.expander(f"{type_emoji.get(search_type, 'ğŸ”')} {query[:50]}{'...' if len(query) > 50 else ''} - {formatted_time}"):
            st.markdown(f"**ğŸ•’ Time:** {formatted_time}")
            st.markdown(f"**ğŸ” Search Type:** {search_type.replace('_', ' ').title()}")
            st.markdown(f"**â“ Query:** {query}")
            st.markdown(f"**ğŸ’¬ Response:** {response}")
            
            if full_length > len(response):
                st.info(f"ğŸ“ Full response was {full_length} characters (truncated for storage)")
            
            # Quick actions
            col1, col2 = st.columns(2)
            with col1:
                if st.button(f"ğŸ”„ Search Again", key=f"research_{i}"):
                    st.session_state.search_query = query
                    st.session_state.current_page = 'search'
                    st.rerun()
            with col2:
                if st.button(f"ğŸ“‹ Copy Query", key=f"copy_{i}"):
                    st.code(query, language="text")
                    st.success("Query copied to code block!")
    
    # Clear history option
    st.markdown("---")
    st.subheader("ğŸ—‘ï¸ Manage History")
    
    col1, col2 = st.columns([1, 3])
    with col1:
        if st.button("ğŸ—‘ï¸ Clear All History", type="secondary"):
            st.session_state.confirm_clear_history = True
    
    # Confirmation dialog
    if getattr(st.session_state, 'confirm_clear_history', False):
        st.warning("âš ï¸ Are you sure you want to clear all your search history? This action cannot be undone.")
        col1, col2, col3 = st.columns([1, 1, 2])
        with col1:
            if st.button("âœ… Yes, Clear", type="primary"):
                try:
                    history_file = os.path.join("history", "history.json")
                    if os.path.exists(history_file):
                        with open(history_file, 'r', encoding='utf-8') as f:
                            history_data = json.load(f)
                        
                        # Clear user's history
                        history_data[st.session_state.username] = []
                        
                        with open(history_file, 'w', encoding='utf-8') as f:
                            json.dump(history_data, f, indent=2, ensure_ascii=False)
                        
                        st.success("âœ… Search history cleared successfully!")
                        st.session_state.confirm_clear_history = False
                        st.rerun()
                    else:
                        st.info("No history file found.")
                except Exception as e:
                    st.error(f"Failed to clear history: {e}")
        
        with col2:
            if st.button("âŒ Cancel"):
                st.session_state.confirm_clear_history = False
                st.rerun()

def validate_password(password):
    """Validate password meets requirements"""
    if len(password) < 8:
        return False, "Password must be at least 8 characters long"
    return True, "Password is valid"

def check_credentials(username, password):
    """Check if username and password are valid"""
    # Simple hardcoded credentials - you can extend this with a database later
    valid_users = {
        "admin": "adminpass",
        "doctor": "doctorpass",
        "user": "userpass123",
        "mediaid": "mediaid2025"
    }
    
    if username in valid_users and valid_users[username] == password:
        return True
    return False

def save_search_history(username: str, query: str, response: str, search_type: str = "search"):
    """Save user search history to JSON file"""
    try:
        # Create history directory if it doesn't exist
        history_dir = "history"
        if not os.path.exists(history_dir):
            os.makedirs(history_dir)
        
        # History file path
        history_file = os.path.join(history_dir, "history.json")
        
        # Load existing history or create new
        if os.path.exists(history_file):
            with open(history_file, 'r', encoding='utf-8') as f:
                history_data = json.load(f)
        else:
            history_data = {}
        
        # Initialize user history if not exists
        if username not in history_data:
            history_data[username] = []
        
        # Create search entry
        search_entry = {
            "timestamp": datetime.now().isoformat(),
            "query": query,
            "response": response[:500] + "..." if len(response) > 500 else response,  # Truncate long responses
            "search_type": search_type,  # 'search', 'upload', 'browse'
            "full_response_length": len(response)
        }
        
        # Add to user history (keep last 100 searches per user)
        history_data[username].append(search_entry)
        if len(history_data[username]) > 100:
            history_data[username] = history_data[username][-100:]
        
        # Save to file
        with open(history_file, 'w', encoding='utf-8') as f:
            json.dump(history_data, f, indent=2, ensure_ascii=False)
            
        return True
        
    except Exception as e:
        st.error(f"Failed to save search history: {e}")
        return False

def load_user_history(username: str) -> List[Dict]:
    """Load search history for a specific user"""
    try:
        history_file = os.path.join("history", "history.json")
        
        if not os.path.exists(history_file):
            return []
        
        with open(history_file, 'r', encoding='utf-8') as f:
            history_data = json.load(f)
        
        return history_data.get(username, [])
        
    except Exception as e:
        st.error(f"Failed to load search history: {e}")
        return []

def render_login_page():
    """Render the login page"""
    st.title("ğŸ¥ MediAid AI - Login")
    st.markdown("Welcome to MediAid AI! Please login to access the medical information system.")
    
    # Center the login form
    col1, col2, col3 = st.columns([1, 2, 1])
    
    with col2:
        st.markdown("### ğŸ” User Login")
        
        # Login form
        with st.form("login_form"):
            username = st.text_input("ğŸ‘¤ Username", placeholder="Enter your username")
            password = st.text_input("ğŸ”’ Password", type="password", placeholder="Enter your password")
            
            submitted = st.form_submit_button("ğŸš€ Login", use_container_width=True)
            
            if submitted:
                if not username:
                    st.error("âŒ Please enter a username")
                elif not password:
                    st.error("âŒ Please enter a password")
                else:
                    # Validate password
                    is_valid, message = validate_password(password)
                    if not is_valid:
                        st.error(f"âŒ {message}")
                    else:
                        # Check credentials
                        if check_credentials(username, password):
                            st.session_state.authenticated = True
                            st.session_state.username = username
                            st.success(f"âœ… Welcome, {username}!")
                            st.balloons()
                            st.rerun()
                        else:
                            st.error("âŒ Invalid username or password")

def main():
    """Main Streamlit app with navigation"""
    
    # Check authentication first
    if not st.session_state.authenticated:
        render_login_page()
        return
    
    # Load medical search systems
    vector_store = load_medical_search()
    llamaindex_query_engine = load_llamaindex_search()
    
    # Store in session state for access across pages
    if 'llamaindex_engine' not in st.session_state:
        st.session_state.llamaindex_engine = llamaindex_query_engine
    
    if not vector_store:
        st.error("Failed to load medical database. Please check the FAISS index file.")
        return
    
    # Render navigation (includes logout button at bottom)
    render_navigation()
    
    # Render current page
    if st.session_state.current_page == 'home':
        render_home_page(vector_store)
    elif st.session_state.current_page == 'search':
        render_search_page(vector_store)
    elif st.session_state.current_page == 'upload':
        render_upload_page(vector_store)
    elif st.session_state.current_page == 'browse':
        render_browse_page(vector_store)
    elif st.session_state.current_page == 'disease_detail':
        render_disease_detail_page(vector_store)
    elif st.session_state.current_page == 'history':
        render_history_page()
    elif st.session_state.current_page == 'faqs':
        render_examples_page(vector_store)
    
    # Footer
    st.markdown("---")
    st.markdown(
        "**MediAid AI** | Medical information from CDC and WHO databases | "
        "âš ï¸ Always consult healthcare professionals for medical advice | "
        "ğŸ”’ Medical queries only - Non-medical content is prohibited"
    )

if __name__ == "__main__":
    main()
